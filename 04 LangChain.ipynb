{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83809f9f-3916-46a5-aee9-ed7295260b77",
   "metadata": {},
   "source": [
    "## LangChain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406f212b-ed41-4916-960a-baed06818880",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00dbaed8-3eec-44d7-ae8c-c22957d0c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, trim_messages\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory, # A messages class for message-list management. messages()\n",
    "    InMemoryChatMessageHistory, # Messages class on top of BaseMessage\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory  # runnable + messages\n",
    "\n",
    "with open('../key.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.readline()\n",
    "    \n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = content.split(' ')[-1]\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"project-test-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511806c2-cada-43d4-b1b7-aaa1d23f3fec",
   "metadata": {},
   "source": [
    "### Base runnable invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b709dc6-a3cc-4358-945c-5f6e43a74a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob! How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 33, 'total_tokens': 47, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None}, id='run-33498634-ba6a-4a66-8efa-c10ca65d66bf-0', usage_metadata={'input_tokens': 33, 'output_tokens': 14, 'total_tokens': 47, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI(model_name = 'gpt-4o-mini')\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"), # 1. Message\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"), # shows that the thread is remembered\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3076a4-27aa-4208-aba7-3d12e9fb5176",
   "metadata": {},
   "source": [
    "### Runnable+MessageHistory invoke\n",
    "Messages:\n",
    "- 1. message = HumanMessage/AIMessage/HumanMessage/trim_messages\n",
    "- 2. messages = [message]\n",
    "  3. ChatMessageHistory:  MessageOps() + messages\n",
    "  4. Prompt = template/builder. Prompt.invoke(variable) = messages.\n",
    "  5. trimmer : trimmer.invoke(messages) = messages\n",
    "\n",
    "Caller:\n",
    "- 1. model: model.invoke(messages)\n",
    "  2. runnableWithHistory: model.invoke(messages, MessageHistory())\n",
    "  3. chain = messages -> prompt | model\n",
    "  4. runnablePassThrough: .assign(key:value) -> output[key]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99faf04b-42bb-4a95-a1aa-1e6e2f0a76f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1060\n"
     ]
    }
   ],
   "source": [
    "# 1. A function to manage sessions. 'id'->'history'\n",
    "#   - InMemoryChatMessageHistory() : init a session\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory: \n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 2. A function to manage 'id'->'history'\n",
    "runnable_with_message_history = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "# 3. Invoke by passing session-id; the runnable class Auto-manage session messages \n",
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "response = runnable_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"I have a black car, but it's dirty now\")],\n",
    "    config=config,\n",
    ")\n",
    "print(len(store['abc2'].messages), len(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0ffa0-fd4f-40c3-bdcd-5802fa58d7aa",
   "metadata": {},
   "source": [
    "### Prompt Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74dbab67-a073-452e-b4ca-d5e860285093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好，Bob！有什么我可以帮助你的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 36, 'total_tokens': 47, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None}, id='run-15cac590-0549-4f4b-bf47-2724659a0a66-0', usage_metadata={'input_tokens': 36, 'output_tokens': 11, 'total_tokens': 47, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "## 1. creates a template. \n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability. Answer only in {Lan}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), \n",
    "    ])\n",
    "# prompt.format(Lan = 'Chinese', messages = store['abc2'].messages) # prompt -> str\n",
    "# prompt.invoke({Lan = 'Chinese', messages = store['abc2'].messages}) # prompt -> promptValue\n",
    "\n",
    "## 2. chain = prompt & invoke\n",
    "chain = prompt | model # both (prompt & model) have .invoke | is overloaded\n",
    "\n",
    "# chain.invoke( {\"messages\": [HumanMessage(content=\"hi! I'm bob\")], \"Lan\": \"Chinese\"})\n",
    "# model.invoke(prompt.format(Lan = 'Chinese', messages = store['abc2'].messages)) # call by message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac0e31e5-d332-4cb3-b26f-13f782792ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='We have exchanged 5 messages so far. How can I assist you further, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 272, 'total_tokens': 290, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-11555d21-4886-4bca-a6f5-e38a87bb32e3-0', usage_metadata={'input_tokens': 272, 'output_tokens': 18, 'total_tokens': 290, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3. runnable = chain & history\n",
    "with_message_history = RunnableWithMessageHistory(chain, \n",
    "                                                  get_session_history,\n",
    "                                                  input_messages_key=\"messages\")\n",
    "\n",
    "with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"How many words have we talked already?\")], \"Lan\": \"Chinese\"},\n",
    "    config = {\"configurable\": {\"session_id\": \"abc2\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4687f-1923-4a7e-b603-9c92e3af99a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. chain + trimmer, runnable + chain\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "## trimmer.invoke(messages)\n",
    "\n",
    "## Order: in={in} -> (itemgetter)in={in}['messages'] -> out=trimmer(in) -> assign({messages=out, in={in})\n",
    "chain = (RunnablePassthrough.assign(messages=(itemgetter(\"messages\") | trimmer))\n",
    "         | prompt | model)\n",
    "# chain.invoke({\"messages\": messages, \"Lan\": \"Chinese\"})\n",
    "\n",
    "## chain -> runnable_history. Order: 'message':get_history() + invoke_message() -> chain\n",
    "runnable = RunnableWithMessageHistory(chain, \n",
    "                                      get_session_history,\n",
    "                                      input_messages_key=\"messages\")\n",
    "\n",
    "runnable.invoke({\"messages\": [HumanMessage(content=\"How to write a good project proposal?\")], \"Lan\": \"Chinese\"},\n",
    "                config = {\"configurable\": {\"session_id\": \"abc2\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfeedb6-18d6-4a07-bfda-b2b3926a5a50",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f68edd0-e573-4afc-a393-c4f845790b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "# model\n",
    "model = ChatOpenAI(model_name = 'gpt-4o-mini')\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState) # Just a dict('messages')\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState): # \n",
    "    response = model.invoke(state[\"messages\"]) \n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e914a345-92d2-478c-aa2c-668304bccceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"user-1\"}}\n",
    "\n",
    "query = \"Hi! I'm Bob.\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387dec83-ab9b-484a-b256-7d5a840726c4",
   "metadata": {},
   "source": [
    "### Prompt + Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb3b1fd2-b88a-4e44-bf26-ea21306b54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "## 1. creates a template. \n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability. Answer only in {Lan}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), \n",
    "    ])\n",
    "\n",
    "## 2. call_model = prompt | model \n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    Lan: str\n",
    "\n",
    "def call_model(state: State): # call model input = {'messages', 'Lan'}\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke(state)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830cddf-d758-41da-bd23-dd1265656a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"What's the weather like today\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"Lan\": language},\n",
    "    config,\n",
    ")\n",
    "# [ m.pretty_print() for m in output[\"messages\"] ]\n",
    " output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39dc573c-5aeb-4c2a-ad9b-805aa1f48fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|支持|图|像|生成|和|绘|制|的|Open|AI|模型|是|D|ALL|-E|。|D|ALL|-E|可以|生成|图|像|，|基|于|文本|描述|创造|视觉|内容|。如果|你|还有|其他|问题|或|需要|进一步|的信息|，请|告诉|我|！||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Which openAI model support image plotting\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b426c77-8dec-4b8a-8306-2fd289c6ed22",
   "metadata": {},
   "source": [
    "### Plot VizGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27a1add4-ac38-4d23-b2d3-4cdce1ac2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dot(graph):\n",
    "    dot_lines = [\"digraph G {\"]\n",
    "    for node in graph.nodes:\n",
    "        dot_lines.append(f'    \"{node}\";')\n",
    "    for edge in graph.edges:\n",
    "        dot_lines.append(f'    \"{edge}\" -> \"{edge[1]}\";')\n",
    "    dot_lines.append(\"}\")\n",
    "    return \"\\n\".join(dot_lines)\n",
    "\n",
    "# Generate DOT representation\n",
    "dot_representation = generate_dot(workflow)\n",
    "\n",
    "with open(\"workflow.dot\", \"w\") as f:\n",
    "    f.write(dot_representation)\n",
    "\n",
    "# dot -Tpng workflow.dot -o workflow.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cd706-5a63-42a6-845f-1bfd3e359969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
